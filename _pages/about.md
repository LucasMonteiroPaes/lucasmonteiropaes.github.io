---
permalink: /
title: "About me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Hi! I am Lucas Monteiro Paes, an AI researcher and mathematician.

I am a Postdoctoral Researcher (AI Resident) at Apple's Machine Learning Research team. I received my PhD in Applied Mathematics from [Harvard University](https://www.seas.harvard.edu), working with Prof. [Flavio Calmon](http://people.seas.harvard.edu/~flavio/#). Previously, I was a Student Researcher at [Google DeepMind](https://deepmind.google/) and an AI Research Scientist Intern at [IBM](https://research.ibm.com/) T.J. Watson Research Center. During my PhD, my research was supported by the [Apple Scholars in AI/ML](https://machinelearning.apple.com/updates/apple-scholars-aiml-2024) Fellowship.

I use theoretical insights to develop safe and trustworthy AI and ML systems. My research is driven by the belief that AI and ML systems should not only be accurate and efficient but also transparent, fair, and aligned with human values and societal norms.

Before joining Harvard, I earned an M.S. in Computational Mathematics and Modeling from Instituto de Matemática Pura e Aplicada ([IMPA](https://impa.br/en_US/)), a beautiful mathematics institute in the [Tijuca National Park](https://en.wikipedia.org/wiki/Tijuca_National_Park) in Rio de Janeiro, Brazil.

## Recent papers
<span style="color: FireBrick"> 
April 2025 - Our paper [AI Alignment at Your Discretion](https://arxiv.org/abs/2502.10441) was accepted at ACM FAccT 2025! Our work also got the [Best Paper Award](https://nenlp.github.io/spr2025/accepted_work.html) at the New England NLP workshop!
</span> \
<span style="font-size:15px">
Our method discovers the values that annotators (humans & LLMs) prioritize using pairwise preferences!
We show that the values LLMs prioritize differ from those of human annotators. 
</span> 

<span style="color: FireBrick"> 
September 2024 - Our paper [Selective Explanations](https://openreview.net/pdf?id=gHCFduRo7o) was accepted at NeurIPS!
</span> \
<span style="font-size:15px">
We introduce Selective Explanations, a method for generating fast and accurate explanations for the predictions of large models. Selective Explanations was developed with an eye towards explaining generative language models like the one we proposed in [MExGen](https://arxiv.org/abs/2403.14459).
</span> 

<span style="color: FireBrick"> 
September 2024 - Our paper [Multi-Group Proportional Representation in Retrieval](https://openreview.net/pdf?id=BRZYhVHvSg) was accepted at NeurIPS!
</span> \
<span style="font-size:15px">
We introduce Multi-Group Proportional Representation (MPR), a metric to measure intersectional representation biases. We also develop an optimized method to perform image retrieval while optimizing MPR. 
</span> 

<span style="color: FireBrick"> 
August 2024 - Our policy brief will be presented at the G20 Meeting [AI Technologies: Algorithmic Monoculture, Arbitrariness, and Global Divides](https://www.t20brasil.org/media/documentos/arquivos/TF05_ST_05_AI_TECHNOLOGIES66cdc9e290631.pdf)!
</span> \
<span style="font-size:15px">
  We discuss the impact of arbitrary predictions when a handful of models are used by the vast majority of the population, E.g., content moderation models of social networks and LLMs.
</span> 

<span style="color: FireBrick"> 
May 2024 - Our paper [Multi-Group Fairness Evaluation via Conditional Value-at-Risk Testing](https://arxiv.org/abs/2312.03867) was published at the IEEE Journal on Selected Areas in Information Theory. 
</span> \
<span style="font-size:15px">
We introduce CVaR fairness, a metric that allows ML practitioners to detect performance disparities across a large number of demographic groups (e.g., all combinations of race, sex, and nationality) with theoretical guarantees.
</span> 

<span style="color: FireBrick"> 
March 2024 - Our paper [Algorithmic Arbitrariness in Content Moderation](https://dl.acm.org/doi/10.1145/3630106.3659036) was accepted at FAccT!
</span> \
<span style="font-size:15px">
In this multidisciplinary paper, we show the prevalence of arbitrary decisions in LLMs trained for content moderation and that these arbitrary decisions disproportionally affect underrepresented communities. Then, we discuss the implications of this finding on (i) freedom of speech, (ii) procedural fairness, and (iii) discrimination.
</span> 

<span style="color: FireBrick"> 
April 2023 - Our paper [On the Inevitability of the Rashomon Effect](https://ieeexplore.ieee.org/document/10206657) was accepted at ISIT 2023.
</span> \
<span style="font-size:15px">
Rashomon effect is the phenomenon where different models achieve the similar permormance but provide different predictions for certain input points.
We show that the Rashomon effect is inevitable and provide a method for practitioners to select the Rashomon parameter as a function of the dataset size. 
</span> 

<span style="color: FireBrick"> 
January 2023 - Our paper [AmnioML: Amniotic Fluid Segmentation and Volume Prediction With Uncertainty Quantification](https://ojs.aaai.org/index.php/AAAI/article/view/26837) received the Innovative Applications of AI award from AAAI. 
</span> \
<span style="font-size:15px"> 
In this paper, we developed an ML solution that combines deep learning and conformal prediction to output fast and accurate volume estimates and segmentation masks from fetal MRI. The proposed solution in the paper was deployed by the biggest clinical diagnosis company in Latin America.
</span> 

<span style="color: FireBrick"> 
August 2022 - Our paper [On the Epistemic Limits of Personalized Prediction](https://proceedings.neurips.cc/paper_files/paper/2022/hash/0cfc9404f89400c5ed897035e0d3748c-Abstract-Conference.html) was accepted at NeurIPS 2022. 
</span> \
<span style="font-size:15px"> 
This paper aims to understand the conditions under which one can detect fair use violations in predictive models and, more interestingly, the conditions where estimating fair use is impossible.
</span> 


## Recent announcements
<span style="color: FireBrick"> 
March 2024 - I am happy to announce that I am joining [Google DeepMind](https://deepmind.google/) as a student researcher!
</span> 

<span style="color: FireBrick"> 
March 2024 - I am thrilled to announce that I was selected as an [Apple Scholar](https://machinelearning.apple.com/updates/apple-scholars-aiml-2024)!
</span> 

<span style="color: FireBrick"> 
May 2023 - I am happy to announce that I am joining [IBM Research](https://research.ibm.com) for the summer. 
</span> 

<span style="color: FireBrick"> 
August 2023 - I received the ISIT Student Travel Grant. 
</span> 

<span style="color: FireBrick"> 
August 2022 - I received the NeurIPS scholar award. 
</span> 

<span style="color: FireBrick"> 
July 2022 - I received the [Fundação Estudar](https://www.estudar.org.br) Leadership Fellowship. 
</span> \
<span style="font-size:15px"> 
The Fellowship aims to support, bring together, and develop Brazil's most promising young leaders that can generate positive transformations in their sector of activity. I was one of the 30 fellows selected out of 33k **(0.08% selected)**. 
</span> 


